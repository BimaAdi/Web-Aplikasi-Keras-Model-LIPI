{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "text = 'menaker menyisir daerah sampai pelosok indonesia'\n",
    "\n",
    "# load the vocabulary -------------------------------------\n",
    "vocab = joblib.load('evaluatePartVocab.joblib')\n",
    "\n",
    "#load tokenizer-------------------------------------------\n",
    "tokenizer = joblib.load('evaluatePartTokenizer.joblib')\n",
    "\n",
    "# load max_length-----------------------------------------------\n",
    "max_length = joblib.load('evaluatePartMax_length.joblib')\n",
    "\n",
    "# load model------------------------------------------------------\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(docs)\n",
    "        #print(encoded)\n",
    "\t# pad sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "\t#print(padded)\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "\t# clean review\n",
    "\tline = clean_doc(review, vocab)\n",
    "\tprint('ini line',line)\n",
    "\t# encode and pad review\n",
    "\tpadded = encode_docs(tokenizer, max_length, [line])\n",
    "    #print('padded',padded)\n",
    "\t# predict sentiment\n",
    "\tyhat = model.predict(padded, verbose=0)\n",
    "\tprint('yhat',yhat)\n",
    "\t# retrieve predicted percentage and label\n",
    "\tpercent_pos = yhat[0,0]\n",
    "\tprint(round(percent_pos))\n",
    "\tif round(percent_pos) == 0:\n",
    "\t\treturn (1-percent_pos), 'NEGATIVE'\n",
    "\treturn percent_pos, 'POSITIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ini line menaker menyisir daerah pelosok indonesia\n",
      "yhat [[ 0.7695756]]\n",
      "1.0\n",
      "Review: [menaker menyisir daerah sampai pelosok indonesia]\n",
      "Sentiment: POSITIVE (76.958%)\n"
     ]
    }
   ],
   "source": [
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alembic==0.9.9\r\n",
      "backcall==0.1.0\r\n",
      "certifi==2018.4.16\r\n",
      "chardet==3.0.4\r\n",
      "decorator==4.3.0\r\n",
      "entrypoints==0.2.3\r\n",
      "idna==2.6\r\n",
      "idnlp==1.0.0\r\n",
      "ipykernel==4.8.2\r\n",
      "ipython==6.3.1\r\n",
      "ipython-genutils==0.2.0\r\n",
      "jedi==0.12.0\r\n",
      "Jinja2==2.10\r\n",
      "jsonschema==2.6.0\r\n",
      "jupyter-client==5.2.3\r\n",
      "jupyter-core==4.4.0\r\n",
      "jupyterhub==0.8.1\r\n",
      "Mako==1.0.7\r\n",
      "MarkupSafe==1.0\r\n",
      "mistune==0.8.3\r\n",
      "nbconvert==5.3.1\r\n",
      "nbformat==4.4.0\r\n",
      "nltk==3.2.2\r\n",
      "notebook==5.4.1\r\n",
      "numpy==1.13.3\r\n",
      "pamela==0.3.0\r\n",
      "pandocfilters==1.4.2\r\n",
      "parso==0.2.0\r\n",
      "pbr==4.0.3\r\n",
      "pexpect==4.5.0\r\n",
      "pickleshare==0.7.4\r\n",
      "prompt-toolkit==1.0.15\r\n",
      "ptyprocess==0.5.2\r\n",
      "Pygments==2.2.0\r\n",
      "python-dateutil==2.7.2\r\n",
      "python-editor==1.0.3\r\n",
      "python-oauth2==1.1.0\r\n",
      "pyzmq==17.0.0\r\n",
      "requests==2.18.4\r\n",
      "Send2Trash==1.5.0\r\n",
      "simplegeneric==0.8.1\r\n",
      "six==1.11.0\r\n",
      "SQLAlchemy==1.2.7\r\n",
      "stevedore==1.28.0\r\n",
      "terminado==0.8.1\r\n",
      "testpath==0.3.1\r\n",
      "tornado==5.0.2\r\n",
      "traitlets==4.3.2\r\n",
      "urllib3==1.22\r\n",
      "virtualenv==16.0.0\r\n",
      "virtualenv-clone==0.3.0\r\n",
      "virtualenvwrapper==4.8.2\r\n",
      "wcwidth==0.1.7\r\n",
      "webencodings==0.5.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop NLP 2018",
   "language": "python",
   "name": "nlp2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
